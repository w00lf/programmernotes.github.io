{"componentChunkName":"component---src-templates-blog-post-js","path":"/taming-europe-routes-3/","result":{"data":{"site":{"siteMetadata":{"title":"Programming notes"}},"markdownRemark":{"id":"358ce8cb-04f9-5fd2-a122-eecfc8edbaea","excerpt":"TL:DR previous parts I was approached by one of my clients with a work that required to parse almost 500 million European routes to see what routes can be…","html":"<h2>TL:DR previous parts</h2>\n<p>I was approached by one of my clients with a work that required to parse almost 500 million European routes to see what routes can be traveled by land transport. After some research, i decided to use graphhopper server application and python celery library, the server was found, scripts were written and i was ready to start the parsing.</p>\n<h2>Celery configuration and tweaks</h2>\n<p>I decided to use celery because of its flexibility and simpleness of configuration. The task was to query graphhopper server, wait for the response, then write the response from workers into the file. http request is a blocking operation that python can greatly optimize. Python does not have full concurrency support because of the <a href=\"https://wiki.python.org/moin/GlobalInterpreterLock\">gil</a> but blocking operations such as this can run simultaneously: when we have http socket waiting for the response python can skip the waiting in the current thread and move the next thread, this way we can have almost the full concurrency. This mechanic will eventually slow down on a large scale because of the CPU load and also slower as the process concurrency model. This is the part where celery flexibility comes in place. We can start celery in multiprocess mode by supplying <code class=\"language-text\">-c</code>(concurrency) flag and supply the number of processes to spawn, that way we will utilize the whole number of server cores. By default, celery will start workers with the process number equal to the number of server` cores. Because i wanted to also utilize the thread concurrency model i started workers a little bit different:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$: <span class=\"token keyword\">for</span> <span class=\"token for-or-select variable\">i</span> <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token number\">1</span><span class=\"token punctuation\">..</span><span class=\"token number\">16</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span> <span class=\"token keyword\">do</span><span class=\"token punctuation\">;</span> celery multi start <span class=\"token variable\">$i</span> -A proj -l INFO -c <span class=\"token number\">3</span> -P threads<span class=\"token punctuation\">;</span> <span class=\"token keyword\">done</span></code></pre></div>\n<p>This command creates 16 celery workers(that’s the number of cores on my server) each with a <code class=\"language-text\">thread</code> concurrency model and 3 worker threads each. Flag <code class=\"language-text\">-P</code> tells celery what concurrency model to use, available options are: prefork (default), eventlet, gevent, solo or threads. I have not tested <code class=\"language-text\">eventlet</code> or <code class=\"language-text\">gevent</code> options because the process -> threads model was known to me before, many web servers use this implementation and it’s great for blocking operations.</p>\n<p>Celery can store the task call inside the broker and retrieve it by call, its not so effective in async processing but perfect for batch data processing with <code class=\"language-text\">celery.group</code> method. This method can accept task name and list of args to call with. Here is the example of processing a list of coordinates with <code class=\"language-text\">celery.group</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">flush_batch_main</span><span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  execution_result <span class=\"token operator\">=</span> celery<span class=\"token punctuation\">.</span>group<span class=\"token punctuation\">(</span>lookup_directions<span class=\"token punctuation\">.</span>s<span class=\"token punctuation\">(</span>\n      row<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> row<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> row <span class=\"token keyword\">in</span> batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> execution_result<span class=\"token punctuation\">]</span></code></pre></div>\n<p>And in the main script i added the following code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">CONCURENCY <span class=\"token operator\">=</span> <span class=\"token number\">500</span>\n\n<span class=\"token keyword\">for</span> row_1 <span class=\"token keyword\">in</span> terms<span class=\"token punctuation\">[</span>START_POINT<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">for</span> row_2 <span class=\"token keyword\">in</span> terms<span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n    row_batch<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>start_point<span class=\"token punctuation\">,</span> end_point<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>row_batch<span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> CONCURENCY<span class=\"token punctuation\">:</span>\n      row_batch<span class=\"token punctuation\">,</span> execution_result <span class=\"token operator\">=</span> flush_batch<span class=\"token punctuation\">(</span>row_batch<span class=\"token punctuation\">)</span>\n      result_batch<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>execution_result<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>result_batch<span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> max_result_batch<span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Flushed'</span><span class=\"token punctuation\">)</span>\n      writer<span class=\"token punctuation\">.</span>writerows<span class=\"token punctuation\">(</span>result_batch<span class=\"token punctuation\">)</span>\n      <span class=\"token keyword\">for</span> entry <span class=\"token keyword\">in</span> result_batch<span class=\"token punctuation\">:</span>\n        hasher<span class=\"token punctuation\">.</span>memorize_route<span class=\"token punctuation\">(</span>entry<span class=\"token punctuation\">[</span><span class=\"token string\">'merged_term'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n      result_batch <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>This code store coordinates inside <code class=\"language-text\">row_batch</code> accumulator until the size of this accumulator surpasses <code class=\"language-text\">CONCURENCY</code> number, after that, it submits the <code class=\"language-text\">row_batch</code> list into the worker pool after all workers complete the tasks it returns the result combined in one list.</p>\n<p>After we process the coordinates batch we need to store this result. This is a blocking operation and if we need to call it often that can lead to a performance decrease. That’s why i decided to first store the results in variable and after that exceeds some number(i decided to go with 10_000) write it into the result file:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>result_batch<span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> max_result_batch<span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Flushed'</span><span class=\"token punctuation\">)</span>\n  writer<span class=\"token punctuation\">.</span>writerows<span class=\"token punctuation\">(</span>result_batch<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> entry <span class=\"token keyword\">in</span> result_batch<span class=\"token punctuation\">:</span>\n    hasher<span class=\"token punctuation\">.</span>memorize_route<span class=\"token punctuation\">(</span>entry<span class=\"token punctuation\">[</span><span class=\"token string\">'merged_term'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  result_batch <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span></code></pre></div>\n<h2>Hungry redis</h2>\n<p>The code was written and optimized, at test runs the server cpu utilization was almost at 100%. I have started the script and left the server running. The pace was great, almost 50 million entries in one day, which gave me 10 days of parsing to complete the task. Then one day i have entered the server and noticed a big spike in memory: an additional 10 Gb was used. The server itself had 64Gb of total memory, graphhopper occupied 35, and scripts took an additional 3. The problem was that memory consumption kept growing. I inspected htop and noticed that it was redis memory consumption. Redis server used 10Gb of memory! So what was the cause of such consumption? I used redis to also store a list of routes that already checked, it was a simple key with string name and value equal to boolean. That way i could memorize what routes were processed and skip it between script restart. As it turns out, a simple key-value store is not optimized in redis and will consume a lot of memory. During my search for the solution i encountered this post: <a href=\"https://instagram-engineering.com/storing-hundreds-of-millions-of-simple-key-value-pairs-in-redis-1091ae80f74c\">storing hundreds of millions of simple key-value pairs in redis</a> which introduced a neat technique that can shred redis memory consumption by a lot. The technique used redis method <a href=\"https://redis.io/commands/hset\">hset</a> instead of the simple <code class=\"language-text\">set</code> key. The way it works is like this: we choose a bucket that will store the value then we store key-value parts in it. Its a simple hashing algorithm, python itself uses it for storing hashed values to avoid collisions. Such store is much more optimized by memory and without speed reduction in access. Even more, i noticed that the keys i used for completed routes were just simple strings so i decided to also hash this string to a hex string that can be easily matched during processing.</p>\n<p>Here is the code i wrote to determine the bucket we will be storing route. I decided to use 500_000 buckets by the number of routes used / 1000, so in each bucket, there will be max 1000 routes stored. python’s <code class=\"language-text\">int</code> method can return int representation of the hex string(that’s the second argument of <code class=\"language-text\">int</code>) which can be used with the remainder of dividing by <code class=\"language-text\">NUMBER_BUCKETS</code> to return a number from 0 to 500_000 depending on hash passed.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">NUMBER_BUCKETS <span class=\"token operator\">=</span> 500_000\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">bucket</span><span class=\"token punctuation\">(</span>hashed_route<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">return</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>hashed_route<span class=\"token punctuation\">,</span> <span class=\"token number\">16</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> NUMBER_BUCKETS</code></pre></div>\n<p>Hash from string can be generated this way in python:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">hash_route</span><span class=\"token punctuation\">(</span>route<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">return</span> hashlib<span class=\"token punctuation\">.</span>md5<span class=\"token punctuation\">(</span>route<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>hexdigest<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>And the code for setting and retrieving keys from redis woth <code class=\"language-text\">hset</code> will look like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">route_exists</span><span class=\"token punctuation\">(</span>route<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  hashed_route <span class=\"token operator\">=</span> hash_route<span class=\"token punctuation\">(</span>route<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> router<span class=\"token punctuation\">.</span>redis_client<span class=\"token punctuation\">.</span>main<span class=\"token punctuation\">.</span>hexists<span class=\"token punctuation\">(</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>bucket<span class=\"token punctuation\">(</span>hashed_route<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> hashed_route<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">memorize_route</span><span class=\"token punctuation\">(</span>route<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  hashed_route <span class=\"token operator\">=</span> hash_route<span class=\"token punctuation\">(</span>route<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> router<span class=\"token punctuation\">.</span>redis_client<span class=\"token punctuation\">.</span>main<span class=\"token punctuation\">.</span>hset<span class=\"token punctuation\">(</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>bucket<span class=\"token punctuation\">(</span>hashed_route<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> hashed_route<span class=\"token punctuation\">,</span> <span class=\"token string\">'1'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>After these optimizations the memory consumption droped by a lot, but still redis consumed much less memory but after next iteration of parsing it still started to increase. I decided to look inside redis for the used keys. One thing i noticed was that there a lot of <code class=\"language-text\">celery*</code> keys inside redis:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$: redis-cli KEYS <span class=\"token string\">\"celery*\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">wc</span> -l\n<span class=\"token number\">500000</span></code></pre></div>\n<p>That’s strange. After some research, i found the reason behind these keys creation. As it turns out, celery can store results into the broker but by default these keys last for one day! I did not need such long time as i was collecting the values right away, the simple tweak of the celery config:</p>\n<div class=\"gatsby-highlight\" data-language=\"diff\"><pre class=\"language-diff\"><code class=\"language-diff\">from __future__ import absolute_import, unicode_literals\nfrom celery import Celery\n\napp = Celery('router',\n<span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span>            broker='redis://',\n<span class=\"token prefix unchanged\"> </span>            backend='redis://',\n<span class=\"token prefix unchanged\"> </span>            include=['router.tasks'])\n</span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span>app.conf.result_expires = 60</span></code></pre></div>\n<p>Fixed the issue and redis memory consumption dropped by a lot! After all 500 million routes parsing the redis server occupied no more than 6 Gb of memory, nice!</p>\n<h2>Retrieving the results</h2>\n<p>Parsing was progressing fast, i had the first results and they were huge. By default i stored the results into a csv file and after parsing just 50 million routes it size exceeded 12 Gb. It was obvious that i won’t have enough space to even store such results and i also needed to parse them through AdWords key planner tool. I need a more optimized by size format to work with and, as it turned out, python has support for a number of marvelous data formats. The easiest one to work from python was <a href=\"https://parquet.apache.org/documentation/latest/\">parquet</a>. Parquet is a binary columnar file format that can store a large array of data and can be easily converted in other formats with the help of pandas. Here is a script that can migrate csv file into the parquet one:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pandas\n<span class=\"token keyword\">import</span> cleaner<span class=\"token punctuation\">.</span>utils\n<span class=\"token keyword\">import</span> sys\n<span class=\"token keyword\">import</span> os\n\nFILE_NAME <span class=\"token operator\">=</span> sys<span class=\"token punctuation\">.</span>argv<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\nCHUNK_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">8000000</span>\nFILE_RESULT_TEMPLATE <span class=\"token operator\">=</span> <span class=\"token string\">'{}_{}.parquet'</span>\nFILE_POSSIBLE_RESULT_TEMPLATE <span class=\"token operator\">=</span> <span class=\"token string\">'{}_{}_possible.parquet'</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">\"__main__\"</span><span class=\"token punctuation\">:</span>\n  i <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n  base_name <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>basename<span class=\"token punctuation\">(</span>FILE_NAME<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n  <span class=\"token keyword\">for</span> chunk <span class=\"token keyword\">in</span> pandas<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>FILE_NAME<span class=\"token punctuation\">,</span> chunksize<span class=\"token operator\">=</span>CHUNK_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>FILE_RESULT_TEMPLATE<span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>base_name<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    cleaned_chunk <span class=\"token operator\">=</span> cleaner<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>clean_routes_dataframe<span class=\"token punctuation\">(</span>chunk<span class=\"token punctuation\">)</span>\n    possible <span class=\"token operator\">=</span> cleaned_chunk<span class=\"token punctuation\">.</span>loc<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>cleaned_chunk<span class=\"token punctuation\">[</span><span class=\"token string\">'distance'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token punctuation\">(</span>cleaned_chunk<span class=\"token punctuation\">[</span><span class=\"token string\">'distance'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> <span class=\"token number\">750000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token punctuation\">(</span>\n        <span class=\"token punctuation\">(</span>cleaned_chunk<span class=\"token punctuation\">[</span><span class=\"token string\">'time'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token punctuation\">(</span>cleaned_chunk<span class=\"token punctuation\">[</span><span class=\"token string\">'time'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> <span class=\"token number\">28800000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n    cleaned_chunk<span class=\"token punctuation\">.</span>to_parquet<span class=\"token punctuation\">(</span>\n        FILE_RESULT_TEMPLATE<span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>base_name<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> engine<span class=\"token operator\">=</span><span class=\"token string\">'pyarrow'</span><span class=\"token punctuation\">)</span>\n    possible<span class=\"token punctuation\">.</span>to_parquet<span class=\"token punctuation\">(</span>\n        FILE_POSSIBLE_RESULT_TEMPLATE<span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>base_name<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> engine<span class=\"token operator\">=</span><span class=\"token string\">'pyarrow'</span><span class=\"token punctuation\">)</span>\n    i <span class=\"token operator\">+=</span> <span class=\"token number\">1</span></code></pre></div>\n<p>I had one large csv file that was storing the parsing results and i need smaller files with no more than 8 million entries that i can easily download, read and convert to the csv and paste into AdWords keywords planner tool. This script will take the csv file name from the command line args(<code class=\"language-text\">sys.argv[1]</code>), open it, read in batches(remember that the original csv file is huge, we need to carefully read it in batches), and then write down a series of parquet files with indexes, no more than 8 million entries. The size optimization was a tremendous one, a 8 million rows parquet file weighted no more than 600 Mb!</p>\n<h2>Conclusions</h2>\n<p>The described architecture model is not perfect, it has a lot of bottlenecks like local requests and waiting for the results to be retrieved from celery. If i will be asked to repeat the job and have more time for the preparations i will use more optimized cli tools like <a href=\"https://github.com/Project-OSRM/osrm-backend\">osrm-backend</a> or <a href=\"https://github.com/valhalla/valhalla\">valhalla</a> but in the end, the job was successfully completed and in a short period of time - just 18 days. Hope you liked these articles and stay tuned for the next ones!</p>","frontmatter":{"title":"Taming Europe routes, part 3","date":"February 14, 2021","description":"How to parse almost 500 million routes with graphhopper server app and python scripts, part 3, actual parsing, redis and python optimizations, parsing speed up"}},"previous":{"fields":{"slug":"/taming-europe-routes-2/"},"frontmatter":{"title":"Taming Europe routes, part 2"}},"next":null},"pageContext":{"id":"358ce8cb-04f9-5fd2-a122-eecfc8edbaea","previousPostId":"428abc62-141a-5e64-8273-da846b7aedb2","nextPostId":null}},"staticQueryHashes":["2511339400","2841359383"]}